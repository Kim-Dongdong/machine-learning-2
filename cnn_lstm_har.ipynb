{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9WbsTSU2-jk"
      },
      "outputs": [],
      "source": [
        "import os, numpy as np, pandas as pd, tensorflow as tf\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, MaxPooling1D, LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "sns.set_context(\"notebook\")\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/UCI_HAR_Dataset/\"\n",
        "\n",
        "def load_file(path):\n",
        "    return pd.read_csv(path, header=None, sep=r\"\\s+\").values\n",
        "\n",
        "def load_group(group, base):\n",
        "    p = os.path.join(base, group, \"Inertial Signals\")\n",
        "    names = [f\"total_acc_x_{group}.txt\", f\"total_acc_y_{group}.txt\", f\"total_acc_z_{group}.txt\",\n",
        "             f\"body_acc_x_{group}.txt\",  f\"body_acc_y_{group}.txt\",  f\"body_acc_z_{group}.txt\",\n",
        "             f\"body_gyro_x_{group}.txt\", f\"body_gyro_y_{group}.txt\", f\"body_gyro_z_{group}.txt\"]\n",
        "    arrays = [load_file(os.path.join(p, n)) for n in names]\n",
        "    return np.dstack(arrays)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(base=BASE_DIR):\n",
        "    Xtr = load_group(\"train\", base); ytr = load_file(os.path.join(base, \"train\", \"y_train.txt\"))\n",
        "    Xte = load_group(\"test\",  base); yte = load_file(os.path.join(base, \"test\",  \"y_test.txt\"))\n",
        "    ytr, yte = ytr - 1, yte - 1\n",
        "    ytr_oh, yte_oh = to_categorical(ytr), to_categorical(yte)\n",
        "    print(Xtr.shape, ytr.shape, ytr_oh.shape, Xte.shape, yte.shape, yte_oh.shape)\n",
        "    return Xtr, ytr, ytr_oh, Xte, yte, yte_oh\n",
        "\n",
        "trainX, trainy_int, trainy, testX, testy_int, testy = load_dataset(BASE_DIR)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    trainX, trainy, test_size=0.2, random_state=SEED, stratify=trainy_int\n",
        ")\n",
        "nT, nC, nY = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
        "print(f\"Input: (T={nT}, C={nC}), Classes={nY}\")"
      ],
      "metadata": {
        "id": "BPz2ZncN3CvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn_lstm(nT, nC, nY):\n",
        "    inp = Input((nT, nC))\n",
        "    x = Conv1D(64, 5, padding='same', activation='relu')(inp)\n",
        "    x = MaxPooling1D(2)(x)                      # (N, 64, 64)\n",
        "    x = Conv1D(128, 3, padding='same', activation='relu')(x)\n",
        "    x = LSTM(64)(x)                             # Conv 출력(3D)을 LSTM에 바로\n",
        "    x = Dropout(0.2)(x)\n",
        "    out = Dense(nY, activation='softmax')(x)\n",
        "    return Model(inp, out, name=\"cnn_lstm\")\n",
        "\n",
        "model = build_cnn_lstm(nT, nC, nY)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "cbs = [\n",
        "    EarlyStopping(patience=5, restore_best_weights=True, monitor='val_accuracy'),\n",
        "    ReduceLROnPlateau(patience=3, factor=0.5, verbose=1, monitor='val_loss')\n",
        "]\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                 epochs=30, batch_size=32, callbacks=cbs, verbose=1)"
      ],
      "metadata": {
        "id": "07YDa4c83Cxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, val_loss = hist.history['loss'], hist.history['val_loss']\n",
        "acc,  val_acc  = hist.history['accuracy'], hist.history['val_accuracy']\n",
        "ep = range(1, len(loss)+1)\n",
        "plt.figure(figsize=(10,3.8))\n",
        "plt.subplot(1,2,1); plt.plot(ep, loss, label='train'); plt.plot(ep, val_loss, label='val'); plt.title('Loss - CNN+LSTM'); plt.legend()\n",
        "plt.subplot(1,2,2); plt.plot(ep, acc,  label='train'); plt.plot(ep, val_acc,  label='val'); plt.title('Accuracy - CNN+LSTM'); plt.legend()\n",
        "plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "xyHzGjdh3Czq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob = model.predict(testX, verbose=0)\n",
        "pred = prob.argmax(axis=-1)\n",
        "acc_test = accuracy_score(testy_int, pred)\n",
        "print(f\"\\n== CNN+LSTM Test Accuracy: {acc_test:.4f}\")\n",
        "print(classification_report(testy_int, pred, digits=4))\n",
        "cm = confusion_matrix(testy_int, pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - CNN+LSTM'); plt.xlabel('Predicted'); plt.ylabel('True')\n",
        "plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "cBeHJAek3Hdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}