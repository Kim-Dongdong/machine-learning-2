{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1fDSTRnwavZQ"
      },
      "outputs": [],
      "source": [
        "# ==== Cell 1: Config & Imports ====\n",
        "import os, math, time, gc, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# 평가/로그 도구(모델 아님)\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 재현성\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "# 경로/데이터 설정\n",
        "DATA_PATH_TEMPLATE = \"/content/drive/MyDrive/Colab Notebooks/mHealth_subject{sid}.log\"\n",
        "\n",
        "SAMPLE_RATE_HZ = 50\n",
        "WIN = 64            # 윈도우 길이(타임스텝)\n",
        "STRIDE = 64         # 스트라이드(겹침 없음)\n",
        "MAJ_THRESH = 0.80   # 다수결 라벨 임계\n",
        "\n",
        "# 채널/클래스\n",
        "IN_CHANNELS = 23    # 1..23 (ECG 포함 2채널은 유지; 논문에선 ECG 사용 X 문구 있지만 여기선 그대로 사용 가능)\n",
        "N_CLASSES  = 12     # 라벨 1..12 (0은 제외)\n",
        "\n",
        "# 모델 하이퍼파라미터\n",
        "CONV_FILTERS = 8\n",
        "KERNEL_SIZE  = 3\n",
        "POOL_SIZE    = 2\n",
        "\n",
        "HIDDEN_UNITS = 32   # BiLSTM 각 방향 hidden size\n",
        "LABEL_SMOOTHING = 0.0\n",
        "\n",
        "# 학습\n",
        "EPOCHS = 3          # 순수 NumPy라면 2~5로 시작 권장\n",
        "BATCH_SIZE = 16\n",
        "LR = 0.005\n",
        "MOMENTUM = 0.9\n",
        "GRAD_CLIP = 5.0\n",
        "WEIGHT_DECAY = 0.0  # L2 (옵션)\n",
        "\n",
        "# CV folds (subject-based)\n",
        "FOLDS = [(1,2), (3,4), (5,6), (7,8), (9,10)]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 2: Load & Merge ====\n",
        "def load_subject_df(sid, path_tmpl=DATA_PATH_TEMPLATE):\n",
        "    path = path_tmpl.format(sid=sid)\n",
        "    df = pd.read_csv(path, sep=r'\\s+', header=None, engine='python')\n",
        "    # 문서 기준: 1..23은 피처, 24는 라벨\n",
        "    df.columns = [f\"f{i}\" for i in range(1,24)] + [\"label\"]\n",
        "    df[\"subject\"] = sid\n",
        "    return df\n",
        "\n",
        "dfs = []\n",
        "for sid in range(1, 11):\n",
        "    df = load_subject_df(sid)\n",
        "    dfs.append(df)\n",
        "full_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "\n",
        "# 라벨 0 제거\n",
        "full_df = full_df[full_df[\"label\"] != 0].reset_index(drop=True)\n",
        "\n",
        "print(\"통합 테이블(라벨!=0) 총 행 수:\", len(full_df))\n",
        "print(full_df.head(3))\n"
      ],
      "metadata": {
        "id": "OS_g_qLggRGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 3: Normalize & Windowing ====\n",
        "# per-subject 표준화(z-score): 각 subject, 각 채널별 평균/표준편차로 정규화 (label 정보는 사용하지 않음)\n",
        "def zscore_per_subject(df, feature_cols):\n",
        "    out = []\n",
        "    for sid, g in df.groupby(\"subject\"):\n",
        "        X = g[feature_cols].values\n",
        "        mu = X.mean(axis=0, keepdims=True)\n",
        "        std = X.std(axis=0, keepdims=True) + 1e-8\n",
        "        Xn = (X - mu) / std\n",
        "        gg = g.copy()\n",
        "        gg[feature_cols] = Xn\n",
        "        out.append(gg)\n",
        "    return pd.concat(out, axis=0, ignore_index=True)\n",
        "\n",
        "FEATURE_COLS = [f\"f{i}\" for i in range(1,24)]\n",
        "full_df_norm = zscore_per_subject(full_df, FEATURE_COLS)\n",
        "\n",
        "def make_windows_for_subject(df_subj, win=WIN, stride=STRIDE, maj_thresh=MAJ_THRESH):\n",
        "    X = df_subj[FEATURE_COLS].values  # (T, 23)\n",
        "    y = df_subj[\"label\"].values       # (T,)\n",
        "    T = len(df_subj)\n",
        "\n",
        "    windows = []\n",
        "    labels  = []\n",
        "    for start in range(0, T - win + 1, stride):\n",
        "        end = start + win\n",
        "        seg_y = y[start:end]\n",
        "        cnt = Counter(seg_y)\n",
        "        maj_label, maj_count = cnt.most_common(1)[0]\n",
        "        if maj_count / win >= maj_thresh:\n",
        "            seg_x = X[start:end, :]\n",
        "            windows.append(seg_x)   # (win, 23)\n",
        "            labels.append(maj_label - 1)  # 1..12 -> 0..11\n",
        "    if not windows:\n",
        "        return np.empty((0, win, X.shape[1]), dtype=np.float32), np.empty((0,), dtype=np.int64)\n",
        "    return np.stack(windows).astype(np.float32), np.array(labels, dtype=np.int64)\n",
        "\n",
        "def build_dataset_for_subjects(subject_ids):\n",
        "    Xs, Ys = [], []\n",
        "    for sid in subject_ids:\n",
        "        df_s = full_df_norm[full_df_norm[\"subject\"] == sid].reset_index(drop=True)\n",
        "        x, y = make_windows_for_subject(df_s)\n",
        "        if len(y) > 0:\n",
        "            Xs.append(x); Ys.append(y)\n",
        "    if not Xs:\n",
        "        return np.empty((0, WIN, len(FEATURE_COLS)), dtype=np.float32), np.empty((0,), dtype=np.int64)\n",
        "    return np.concatenate(Xs, axis=0), np.concatenate(Ys, axis=0)\n",
        "\n",
        "# 빠른 sanity check\n",
        "trX, trY = build_dataset_for_subjects([1,2,3])\n",
        "tsX, tsY = build_dataset_for_subjects([4])\n",
        "print(\"예시 shapes:\", trX.shape, trY.shape, tsX.shape, tsY.shape)\n"
      ],
      "metadata": {
        "id": "hJwAlDlbgwun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 4: Utils (one-hot, batching, loss etc.) ====\n",
        "def one_hot(y, num_classes):\n",
        "    oh = np.zeros((len(y), num_classes), dtype=np.float32)\n",
        "    oh[np.arange(len(y)), y] = 1.0\n",
        "    return oh\n",
        "\n",
        "def smooth_labels(y_onehot, eps=LABEL_SMOOTHING):\n",
        "    if eps <= 0.0:\n",
        "        return y_onehot\n",
        "    K = y_onehot.shape[1]\n",
        "    return (1 - eps) * y_onehot + eps / K\n",
        "\n",
        "def iterate_minibatches(X, y, batch_size, shuffle=True):\n",
        "    n = len(y)\n",
        "    idx = np.arange(n)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx)\n",
        "    for i in range(0, n, batch_size):\n",
        "        sel = idx[i:i+batch_size]\n",
        "        yield X[sel], y[sel]\n",
        "\n",
        "def softmax(z):\n",
        "    z = z - z.max(axis=1, keepdims=True)\n",
        "    ez = np.exp(z)\n",
        "    return ez / (ez.sum(axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "def cross_entropy(pred, target_onehot):\n",
        "    # pred: (B, K) softmax prob, target: onehot\n",
        "    eps = 1e-12\n",
        "    return -np.sum(target_onehot * np.log(pred + eps), axis=1).mean()\n",
        "\n",
        "def accuracy(pred_class, y):\n",
        "    return (pred_class == y).mean()\n"
      ],
      "metadata": {
        "id": "X7AEtnJGgww8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 5: Conv1D & MaxPool1D (from scratch) ====\n",
        "class Conv1D:\n",
        "    # Numpy Conv1D with padding='valid'\n",
        "    def __init__(self, in_ch, out_ch, ksize):\n",
        "        self.in_ch = in_ch\n",
        "        self.out_ch = out_ch\n",
        "        self.ksize = ksize\n",
        "        # Xavier-like init\n",
        "        lim = math.sqrt(6.0 / (in_ch*ksize + out_ch))\n",
        "        self.W = np.random.uniform(-lim, lim, size=(out_ch, in_ch, ksize)).astype(np.float32)\n",
        "        self.b = np.zeros((out_ch,), dtype=np.float32)\n",
        "        # grads\n",
        "        self.gW = np.zeros_like(self.W)\n",
        "        self.gb = np.zeros_like(self.b)\n",
        "        # cache\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "        K = self.ksize\n",
        "        OC = self.out_ch\n",
        "        out_T = T - K + 1\n",
        "        y = np.zeros((B, out_T, OC), dtype=np.float32)\n",
        "        # naive conv (can be sped up by im2col)\n",
        "        for b in range(B):\n",
        "            for t in range(out_T):\n",
        "                xt = x[b, t:t+K, :]             # (K, C)\n",
        "                # (OC, IC, K) dot (IC,K)->OC\n",
        "                # rearrange xt -> (IC,K)\n",
        "                xt2 = xt.T                       # (C, K)\n",
        "                y[b, t, :] = (self.W.reshape(OC, -1) @ xt2.reshape(-1)) + self.b\n",
        "        self.cache = x\n",
        "        return y\n",
        "\n",
        "    def backward(self, dy):\n",
        "        # dy: (B, T_out, OC)\n",
        "        x = self.cache\n",
        "        B, T, C = x.shape\n",
        "        OC, IC, K = self.W.shape\n",
        "        T_out = dy.shape[1]\n",
        "        dx = np.zeros_like(x)\n",
        "        self.gW.fill(0.0); self.gb.fill(0.0)\n",
        "\n",
        "        for b in range(B):\n",
        "            for t in range(T_out):\n",
        "                # grad w.r.t b\n",
        "                self.gb += dy[b, t, :]\n",
        "                # grad w.r.t W\n",
        "                xt = x[b, t:t+K, :].T           # (IC,K)\n",
        "                # add outer: (OC,1)*(1,IC*K)\n",
        "                self.gW += dy[b, t, :].reshape(OC,1,1) * xt.reshape(1,IC,K)\n",
        "                # grad w.r.t x\n",
        "                # dL/dx_segment = sum_oc dy * W[oc]\n",
        "                for oc in range(OC):\n",
        "                    dx[b, t:t+K, :] += (dy[b, t, oc] * self.W[oc].T)  # (K,IC) add\n",
        "        return dx\n",
        "\n",
        "class ReLU:\n",
        "    def __init__(self): self.mask = None\n",
        "    def forward(self, x):\n",
        "        self.mask = x > 0\n",
        "        return x * self.mask\n",
        "    def backward(self, dy):\n",
        "        return dy * self.mask\n",
        "\n",
        "class MaxPool1D:\n",
        "    def __init__(self, pool_size):\n",
        "        self.pool = pool_size\n",
        "        self.cache = None\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "        P = self.pool\n",
        "        assert T % P == 0, \"Time length must be divisible by pool size\"\n",
        "        out_T = T // P\n",
        "        y = np.zeros((B, out_T, C), dtype=np.float32)\n",
        "        self.cache = (x, np.zeros_like(x, dtype=bool))\n",
        "        _, mask = self.cache\n",
        "        for t in range(out_T):\n",
        "            seg = x[:, t*P:(t+1)*P, :]  # (B, P, C)\n",
        "            m = seg.max(axis=1)         # (B, C)\n",
        "            y[:, t, :] = m\n",
        "            # mask for backprop\n",
        "            maxpos = (seg == m[:, None, :])\n",
        "            mask[:, t*P:(t+1)*P, :] = maxpos\n",
        "        return y\n",
        "    def backward(self, dy):\n",
        "        # dy: (B, T_out, C)\n",
        "        x, mask = self.cache\n",
        "        B, T, C = x.shape\n",
        "        P = self.pool\n",
        "        out_T = dy.shape[1]\n",
        "        dx = np.zeros_like(x)\n",
        "        for t in range(out_T):\n",
        "            dx[:, t*P:(t+1)*P, :] += dy[:, t, :][:, None, :] * mask[:, t*P:(t+1)*P, :]\n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "fgR2dKdYgwzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 6: LSTM & BiLSTM (from scratch) ====\n",
        "def sigmoid(x): return 1.0 / (1.0 + np.exp(-x))\n",
        "def dsigmoid(y): return y * (1 - y)           # y = sigmoid(x)\n",
        "def dtanh(y): return 1 - y*y                   # y = tanh(x)\n",
        "\n",
        "class LSTM:\n",
        "    \"\"\"\n",
        "    단일 방향 LSTM (many-to-many): 입력 (B,T,Din) -> 출력 (B,T,H)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        self.D = input_dim\n",
        "        self.H = hidden_dim\n",
        "        # Weights: x->(i,f,o,g), h->(i,f,o,g)\n",
        "        # concat for speed: [xi, xf, xo, xg]\n",
        "        lim = math.sqrt(1.0/(input_dim+hidden_dim))\n",
        "        self.Wx = np.random.uniform(-lim, lim, size=(input_dim, 4*hidden_dim)).astype(np.float32)\n",
        "        self.Wh = np.random.uniform(-lim, lim, size=(hidden_dim, 4*hidden_dim)).astype(np.float32)\n",
        "        self.b  = np.zeros((4*hidden_dim,), dtype=np.float32)\n",
        "\n",
        "        # grads\n",
        "        self.gWx = np.zeros_like(self.Wx)\n",
        "        self.gWh = np.zeros_like(self.Wh)\n",
        "        self.gb  = np.zeros_like(self.b)\n",
        "\n",
        "        # cache for BPTT\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "        H = self.H\n",
        "        h = np.zeros((B, H), dtype=np.float32)\n",
        "        c = np.zeros((B, H), dtype=np.float32)\n",
        "        hs = np.zeros((B, T, H), dtype=np.float32)\n",
        "\n",
        "        self.cache = {\"x\": x, \"h\": [None]*(T+1), \"c\": [None]*(T+1), \"gates\": [None]*T}\n",
        "        self.cache[\"h\"][0] = h.copy()\n",
        "        self.cache[\"c\"][0] = c.copy()\n",
        "\n",
        "        for t in range(T):\n",
        "            xt = x[:, t, :]                    # (B,D)\n",
        "            a = xt @ self.Wx + h @ self.Wh + self.b  # (B,4H)\n",
        "            ai, af, ao, ag = np.split(a, 4, axis=1)\n",
        "            it = sigmoid(ai)\n",
        "            ft = sigmoid(af)\n",
        "            ot = sigmoid(ao)\n",
        "            gt = np.tanh(ag)\n",
        "            c = ft * c + it * gt\n",
        "            h = ot * np.tanh(c)\n",
        "            hs[:, t, :] = h\n",
        "\n",
        "            self.cache[\"h\"][t+1] = h.copy()\n",
        "            self.cache[\"c\"][t+1] = c.copy()\n",
        "            self.cache[\"gates\"][t] = (it, ft, ot, gt, ai, af, ao, ag)\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        # dhs: (B,T,H) grads wrt outputs\n",
        "        x = self.cache[\"x\"]\n",
        "        B, T, D = x.shape\n",
        "        H = self.H\n",
        "\n",
        "        self.gWx.fill(0.0); self.gWh.fill(0.0); self.gb.fill(0.0)\n",
        "        dx = np.zeros_like(x)\n",
        "        dh_next = np.zeros((B, H), dtype=np.float32)\n",
        "        dc_next = np.zeros((B, H), dtype=np.float32)\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "            it, ft, ot, gt, ai, af, ao, ag = self.cache[\"gates\"][t]\n",
        "            h_prev = self.cache[\"h\"][t]\n",
        "            c_prev = self.cache[\"c\"][t]\n",
        "            c_cur  = self.cache[\"c\"][t+1]\n",
        "\n",
        "            dh = dhs[:, t, :] + dh_next\n",
        "            do = dh * np.tanh(c_cur)\n",
        "            dco = dh * ot * (1 - np.tanh(c_cur)**2) + dc_next\n",
        "            df = dco * c_prev\n",
        "            di = dco * gt\n",
        "            dg = dco * it\n",
        "\n",
        "            dai = di * dsigmoid(it)\n",
        "            daf = df * dsigmoid(ft)\n",
        "            dao = do * dsigmoid(ot)\n",
        "            dag = dg * dtanh(gt)\n",
        "\n",
        "            da = np.concatenate([dai, daf, dao, dag], axis=1)   # (B,4H)\n",
        "\n",
        "            self.gb += da.sum(axis=0)\n",
        "            self.gWx += x[:, t, :].T @ da\n",
        "            self.gWh += h_prev.T @ da\n",
        "\n",
        "            dx[:, t, :] = da @ self.Wx.T\n",
        "            dh_next = da @ self.Wh.T\n",
        "            dc_next = dco * ft\n",
        "\n",
        "        return dx\n",
        "\n",
        "class BiLSTM:\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        self.fwd = LSTM(input_dim, hidden_dim)\n",
        "        self.bwd = LSTM(input_dim, hidden_dim)\n",
        "        self.H = hidden_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,T,D)\n",
        "        y_f = self.fwd.forward(x)                # (B,T,H)\n",
        "        y_b = self.bwd.forward(x[:, ::-1, :])    # (B,T,H) on reversed\n",
        "        y_b = y_b[:, ::-1, :]                    # align time\n",
        "        return np.concatenate([y_f, y_b], axis=2)  # (B,T,2H)\n",
        "\n",
        "    def backward(self, dy):\n",
        "        # dy: (B,T,2H)\n",
        "        B, T, _ = dy.shape\n",
        "        H = self.H\n",
        "        dy_f = dy[:, :, :H]\n",
        "        dy_b = dy[:, :, H:]\n",
        "\n",
        "        dx_f = self.fwd.backward(dy_f)               # (B,T,D)\n",
        "        # backward for reversed sequence\n",
        "        dx_b_rev = self.bwd.backward(dy_b[:, ::-1, :])  # grads on reversed inputs\n",
        "        dx_b = dx_b_rev[:, ::-1, :]\n",
        "        return dx_f + dx_b\n",
        "\n",
        "    @property\n",
        "    def params_and_grads(self):\n",
        "        return [\n",
        "            (self.fwd.Wx, self.fwd.gWx), (self.fwd.Wh, self.fwd.gWh), (self.fwd.b, self.fwd.gb),\n",
        "            (self.bwd.Wx, self.bwd.gWx), (self.bwd.Wh, self.bwd.gWh), (self.bwd.b, self.bwd.gb),\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "-P4NJNyIgw1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 7: Dense & Model Compose ====\n",
        "class Dense:\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        lim = math.sqrt(6.0/(in_dim+out_dim))\n",
        "        self.W = np.random.uniform(-lim, lim, size=(in_dim, out_dim)).astype(np.float32)\n",
        "        self.b = np.zeros((out_dim,), dtype=np.float32)\n",
        "        self.gW = np.zeros_like(self.W)\n",
        "        self.gb = np.zeros_like(self.b)\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,T,Din) -> y: (B,T,Dout)\n",
        "        y = x @ self.W + self.b\n",
        "        self.cache = x\n",
        "        return y\n",
        "\n",
        "    def backward(self, dy):\n",
        "        # dy: (B,T,Dout)\n",
        "        x = self.cache\n",
        "        B, T, D = x.shape\n",
        "        self.gW.fill(0.0); self.gb.fill(0.0)\n",
        "        # sum over B,T\n",
        "        X2 = x.reshape(B*T, D)\n",
        "        dY2 = dy.reshape(B*T, -1)\n",
        "        self.gW += X2.T @ dY2\n",
        "        self.gb += dY2.sum(axis=0)\n",
        "        dx = dy @ self.W.T\n",
        "        return dx\n",
        "\n",
        "class CNN_BiLSTM_Classifier:\n",
        "    def __init__(self, in_channels, conv_filters, kernel_size, pool_size, lstm_hidden, n_classes):\n",
        "        self.conv = Conv1D(in_channels, conv_filters, kernel_size)\n",
        "        self.relu = ReLU()\n",
        "        self.pool = MaxPool1D(pool_size)\n",
        "        # BiLSTM 입력 차원은 conv의 출력 채널 수\n",
        "        self.bilstm = BiLSTM(conv_filters, lstm_hidden)\n",
        "        self.fc = Dense(2*lstm_hidden, n_classes)  # time-distributed\n",
        "\n",
        "        # 옵티마 상태 (SGD+momentum)\n",
        "        self.opt_v = {}\n",
        "        for p, g in self.params_and_grads:\n",
        "            self.opt_v[id(p)] = np.zeros_like(p)\n",
        "\n",
        "    @property\n",
        "    def params_and_grads(self):\n",
        "        items = [\n",
        "            (self.conv.W, self.conv.gW), (self.conv.b, self.conv.gb),\n",
        "            (self.bilstm.fwd.Wx, self.bilstm.fwd.gWx), (self.bilstm.fwd.Wh, self.bilstm.fwd.gWh), (self.bilstm.fwd.b, self.bilstm.fwd.gb),\n",
        "            (self.bilstm.bwd.Wx, self.bilstm.bwd.gWx), (self.bilstm.bwd.Wh, self.bilstm.bwd.gWh), (self.bilstm.bwd.b, self.bilstm.bwd.gb),\n",
        "            (self.fc.W, self.fc.gW), (self.fc.b, self.fc.gb)\n",
        "        ]\n",
        "        return items\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,T,C_in)\n",
        "        y = self.conv.forward(x)             # (B, T-K+1, F)\n",
        "        y = self.relu.forward(y)             # (B, T', F)\n",
        "        y = self.pool.forward(y)             # (B, T'', F)\n",
        "        y = self.bilstm.forward(y)           # (B, T'', 2H)\n",
        "        y = self.fc.forward(y)               # (B, T'', K)\n",
        "        return y\n",
        "\n",
        "    def backward(self, dlogits):\n",
        "        dy = self.fc.backward(dlogits)       # (B,T'',2H)\n",
        "        dy = self.bilstm.backward(dy)        # (B,T'',F)\n",
        "        dy = self.pool.backward(dy)          # (B,T',F)\n",
        "        dy = self.relu.backward(dy)          # (B,T',F)\n",
        "        dx = self.conv.backward(dy)          # (B,T,C)\n",
        "        return dx\n",
        "\n",
        "    def sgd_step(self, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, clip=GRAD_CLIP):\n",
        "        # optional weight decay\n",
        "        for p, g in self.params_and_grads:\n",
        "            if weight_decay > 0 and p.ndim >= 2:\n",
        "                g += weight_decay * p\n",
        "\n",
        "        # global grad clipping (by value)\n",
        "        if clip is not None:\n",
        "            for _, g in self.params_and_grads:\n",
        "                np.clip(g, -clip, clip, out=g)\n",
        "\n",
        "        # momentum update\n",
        "        for p, g in self.params_and_grads:\n",
        "            v = self.opt_v[id(p)]\n",
        "            v[:] = momentum * v - lr * g\n",
        "            p += v\n",
        "\n",
        "    def zero_grads(self):\n",
        "        for _, g in self.params_and_grads:\n",
        "            g.fill(0.0)\n"
      ],
      "metadata": {
        "id": "SJqjeI-rg2qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 8: Train & Evaluate ====\n",
        "def train_one_epoch(model, X, y, batch_size=BATCH_SIZE):\n",
        "    model.zero_grads()\n",
        "    losses = []\n",
        "    accs = []\n",
        "    for xb, yb in iterate_minibatches(X, y, batch_size, shuffle=True):\n",
        "        # fwd\n",
        "        logits_seq = model.forward(xb)           # (B,T_out,K)\n",
        "        logits = logits_seq[:, -1, :]            # 마지막 타임스텝으로 시퀀스 분류\n",
        "        probs  = softmax(logits)\n",
        "        y_onehot = one_hot(yb, N_CLASSES)\n",
        "        y_s = smooth_labels(y_onehot, LABEL_SMOOTHING)\n",
        "\n",
        "        loss = cross_entropy(probs, y_s)\n",
        "        losses.append(loss)\n",
        "        accs.append(accuracy(np.argmax(probs, axis=1), yb))\n",
        "\n",
        "        # bwd: dL/dlogits = probs - target\n",
        "        dlogits = (probs - y_s) / len(yb)       # 평균의 도함수\n",
        "        # 확장해서 시퀀스 형태로 (마지막 타임스텝만 nonzero)\n",
        "        dlogits_seq = np.zeros_like(logits_seq)\n",
        "        dlogits_seq[:, -1, :] = dlogits\n",
        "\n",
        "        model.zero_grads()\n",
        "        model.backward(dlogits_seq)\n",
        "        model.sgd_step()\n",
        "\n",
        "    return float(np.mean(losses)), float(np.mean(accs))\n",
        "\n",
        "def evaluate(model, X, y, batch_size=256, verbose=False):\n",
        "    preds = []\n",
        "    for i in range(0, len(y), batch_size):\n",
        "        xb = X[i:i+batch_size]\n",
        "        logits_seq = model.forward(xb)\n",
        "        logits = logits_seq[:, -1, :]\n",
        "        probs = softmax(logits)\n",
        "        preds.append(np.argmax(probs, axis=1))\n",
        "    y_pred = np.concatenate(preds)\n",
        "    acc = accuracy_score(y, y_pred)\n",
        "    if verbose:\n",
        "        print(classification_report(y, y_pred, digits=4))\n",
        "        print(\"Confusion matrix:\\n\", confusion_matrix(y, y_pred))\n",
        "    return acc, y_pred\n"
      ],
      "metadata": {
        "id": "nHuOAyZJg2sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 9: 5-Fold Subject-wise CV ====\n",
        "def fit_fold(train_sids, test_sids,\n",
        "             epochs=EPOCHS,\n",
        "             conv_filters=CONV_FILTERS,\n",
        "             kernel_size=KERNEL_SIZE,\n",
        "             pool_size=POOL_SIZE,\n",
        "             hidden_units=HIDDEN_UNITS):\n",
        "    # 데이터 구성\n",
        "    Xtr, Ytr = build_dataset_for_subjects(train_sids)\n",
        "    Xte, Yte = build_dataset_for_subjects(test_sids)\n",
        "\n",
        "    print(f\"Train windows: {len(Ytr)} | Test windows: {len(Yte)}\")\n",
        "    if len(Ytr) == 0 or len(Yte) == 0:\n",
        "        print(\"데이터가 비어 fold를 건너뜁니다.\")\n",
        "        return None\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = CNN_BiLSTM_Classifier(IN_CHANNELS, conv_filters, kernel_size,\n",
        "                                  pool_size, hidden_units, N_CLASSES)\n",
        "\n",
        "    # 학습 루프\n",
        "    for ep in range(1, epochs+1):\n",
        "        t0 = time.time()\n",
        "        loss, tr_acc = train_one_epoch(model, Xtr, Ytr, BATCH_SIZE)\n",
        "        te_acc, _ = evaluate(model, Xte, Yte, verbose=False)\n",
        "        dt = time.time()-t0\n",
        "        print(f\"  Epoch {ep:02d} | loss={loss:.4f} | train_acc={tr_acc*100:.2f}% | test_acc={te_acc*100:.2f}%\")\n",
        "    return model, (Xte, Yte)\n",
        "\n",
        "all_results = []\n",
        "for fi, test_pair in enumerate([(1,2), (3,4), (5,6), (7,8), (9,10)], start=1):\n",
        "    test_sids = list(test_pair)\n",
        "    train_sids = [sid for sid in range(1,11) if sid not in test_sids]\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\"Fold {fi} | Test subjects = {tuple(test_sids)}\")\n",
        "    out = fit_fold(train_sids, test_sids)\n",
        "    if out is None:\n",
        "        all_results.append(np.nan)\n",
        "        continue\n",
        "    model, (Xte, Yte) = out\n",
        "    te_acc, ypred = evaluate(model, Xte, Yte, verbose=True)\n",
        "    print(f\"Fold {fi} final Test Acc = {te_acc*100:.2f}%\")\n",
        "    all_results.append(te_acc)\n",
        "    # 메모리 정리(다음 fold 속도)\n",
        "    del model, Xte, Yte; gc.collect()\n",
        "\n",
        "print(\"\\n==== 5-Fold Summary ====\")\n",
        "for i, a in enumerate(all_results, 1):\n",
        "    print(f\"Fold {i}: {a*100:.2f}%\" if not np.isnan(a) else f\"Fold {i}: N/A\")\n",
        "print(f\"Mean Acc: {np.nanmean(all_results)*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "RMy8RlMGg6KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 10: Tuning Tips (Optional) ====\n",
        "# 1) 속도 ↑: HIDDEN_UNITS, CONV_FILTERS를 더 작게 (예: 16, 6)\n",
        "# 2) 일반화: LABEL_SMOOTHING = 0.05 ~ 0.1 시도\n",
        "# 3) 윈도우 더 짧게: WIN=48 (단, POOL_SIZE로 나눠떨어지게)\n",
        "# 4) 학습 안정: LR=0.003, MOMENTUM=0.9 유지, GRAD_CLIP=5.0 유지\n",
        "# 5) 시퀀스 표준화: 현재 per-subject z-score, 필요시 per-log 전체 z-score로 변경 가능\n",
        "pass"
      ],
      "metadata": {
        "id": "vJqMWo9Rg6MQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}